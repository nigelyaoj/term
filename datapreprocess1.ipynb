{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./train.csv\")\n",
    "test_data = pd.read_csv(\"./sample_submission.csv\")\n",
    "upper_cases = [chr(ord(\"A\") + val) for val in range(26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_ids = list(set(train_data[\"Id\"]))\n",
    "id_title_dict = dict()\n",
    "for tid in title_ids:\n",
    "    id_title_dict[tid] = pd.read_json(os.path.join(\"./train/\", tid+\".json\"))\n",
    "    \n",
    "# id_title_dict[\"0007f880-0a9b-492d-9a58-76eb0b0e0bd7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def find_is_datalabel(datasets, can_dataset):\n",
    "    for target in datasets:\n",
    "        if can_dataset.find(target) > -1 or target.find(can_dataset) > -1:\n",
    "            return 1 \n",
    "    return 0\n",
    "\n",
    "id_datalabel_dict = dict()\n",
    "for id_, dataset_ in zip(train_data[\"Id\"], train_data[\"dataset_label\"]):\n",
    "    id_datalabel_dict.setdefault(id_, []).append(dataset_)\n",
    "\n",
    "    \n",
    "def find_big_case(word):\n",
    "    if word.find(\"CoV\") > -1 or word.find(\"COV\") > -1:\n",
    "        return True\n",
    "    \n",
    "    for l in word:\n",
    "        if l not in upper_cases:\n",
    "            return False\n",
    "    \n",
    "    return (len(word) >= 4)\n",
    "\n",
    "\n",
    "def find_candidate_sen(text):\n",
    "#     print(text)\n",
    "#     text = \"69 The Baltimore Longitudinal Study of Aging (BLSA) recently reported that the association between AD-related pathology and IR could not be confirmed.\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    words = text.split(\" \")\n",
    "    candidate_sentences = []\n",
    "    candidate_datasets = []\n",
    "    sentence = []\n",
    "    can_dataset = []\n",
    "    flag = 0\n",
    "    record = False\n",
    "    patience = 1\n",
    "    flag_st = flag_ed = None\n",
    "    for word_id, word in enumerate(words):\n",
    "        if find_big_case(word):\n",
    "#             print(word)\n",
    "            record = True\n",
    "            can_dataset.append(word)\n",
    "#             can_dataset.append(\" \".join(words[max(0, word_id-5):word_id+5]))\n",
    "#             print(\" \".join(words[max(0, word_id-5):word_id+5]))\n",
    "        sentence.append(word)\n",
    "      \n",
    "        if len(word) > 0 and word[0] in upper_cases:\n",
    "            patience = 1\n",
    "            flag += 1\n",
    "            if flag == 1:\n",
    "                flag_st = word_id\n",
    "#                 print(word)\n",
    "            if flag == 2:\n",
    "                record = True\n",
    "                  \n",
    "        else:\n",
    "            if patience == 0:\n",
    "                if flag > 1:\n",
    "                    flag_ed = word_id\n",
    "#                     print(\" \".join(words[flag_st:flag_ed]))\n",
    "                    can_dataset.append(\" \".join(words[flag_st:flag_ed]))\n",
    "                flag = 0\n",
    "                patience = 1\n",
    "            else:\n",
    "                patience -= 1\n",
    "#         print(word)\n",
    "\n",
    "        if word.endswith(\",\") or word.endswith(\":\") or word.endswith(\";\"):\n",
    "            if flag > 1:\n",
    "                flag_ed = word_id\n",
    "                can_dataset.append(\" \".join(words[flag_st:flag_ed+1]))\n",
    "            flag = 0\n",
    "            patience = 1\n",
    "        if len(word) > 0 and word[-1] in [\".\", \"?\", \"!\"] and (len(word) < 2 or (word[-2] not in upper_cases)):\n",
    "            \n",
    "            if record:\n",
    "                candidate_sentences.append(\" \".join(sentence))\n",
    "                candidate_datasets.append(can_dataset)\n",
    "            can_dataset = []\n",
    "            sentence = []\n",
    "            record = False\n",
    "            flag = 0\n",
    "            patience = 1\n",
    "        \n",
    "#     s=2\n",
    "    return candidate_sentences, candidate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3287/14316 [03:53<14:40, 12.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55f1cc31-58e6-4ca0-b8bf-bc80bfecec09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14316/14316 [17:11<00:00, 13.89it/s]\n"
     ]
    }
   ],
   "source": [
    "id_dfs_dict = dict()\n",
    "for tid in tqdm(title_ids):\n",
    "    dfs = []\n",
    "    total_row_num = id_title_dict[tid].shape[0]\n",
    "    for row_id in range(total_row_num):\n",
    "        candidate_sentences, candidate_datasets = find_candidate_sen(id_title_dict[tid][\"text\"][row_id])\n",
    "#         print(candidate_sentences, candidate_datasets)\n",
    "        if len(candidate_sentences)>0:\n",
    "            pd_sentences, pd_datasets, labels = [], [], []\n",
    "            for idx in range(len(candidate_sentences)):\n",
    "                for dataset_ in candidate_datasets[idx]:\n",
    "                    pd_sentences.append(candidate_sentences[idx])\n",
    "                    pd_datasets.append(dataset_)\n",
    "#                     print(candidate_sentences[idx], dataset_)\n",
    "            \n",
    "                    labels.append(find_is_datalabel(id_datalabel_dict[tid], dataset_))\n",
    "            df = pd.DataFrame()\n",
    "            df[\"Label\"] = labels\n",
    "            df[\"Sentence\"] = pd_sentences\n",
    "            df[\"Can_data\"] = pd_datasets\n",
    "            df[\"Section\"] = id_title_dict[tid][\"section_title\"][row_id]\n",
    "            df[\"Id\"] = tid\n",
    "            dfs.append(df)\n",
    "    \n",
    "    if dfs:\n",
    "        dfs = pd.concat(dfs, axis=0)  \n",
    "        id_dfs_dict[tid] = dfs\n",
    "    else:\n",
    "        print(tid)\n",
    "\n",
    "can_dataset_extract = pd.concat(list(id_dfs_dict.values()), axis=0)\n",
    "\n",
    "sen_len = [len(val.split(\" \")) for val in can_dataset_extract[\"Sentence\"]]\n",
    "can_dataset_extract[\"sen_length\"] = sen_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(can_dataset_extract[\"sen_length\"], bins=10)\n",
    "# a = set(can_dataset_extract[can_dataset_extract[\"sen_length\"]>500][\"Sentence\"].values)\n",
    "can_dataset_extract = can_dataset_extract[can_dataset_extract[\"sen_length\"]<200]\n",
    "can_dataset_extract.drop(labels=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "can_dataset_extract.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "filters = (can_dataset_extract[\"Label\"] == 0).values & (can_dataset_extract[\"sen_length\"]>100).values\n",
    "can_dataset_extract = can_dataset_extract[~filters]\n",
    "can_dataset_extract = can_dataset_extract[~can_dataset_extract[\"Can_data\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1392462, 6) 0.04511433705192673\n",
      "(348116, 6) 0.04532971768031346\n"
     ]
    }
   ],
   "source": [
    "# save data\n",
    "\n",
    "can_dataset_extract.to_csv(\"data1.csv\")\n",
    "\n",
    "full_data = can_dataset_extract.sample(frac=1)\n",
    "train_num = int(full_data.shape[0] * 0.8)\n",
    "train_data = full_data.iloc[:train_num, :]\n",
    "valid_data = full_data.iloc[train_num:, :]\n",
    "\n",
    "train_data.to_csv(\"data1_train.csv\")\n",
    "valid_data.to_csv(\"data1_val.csv\")\n",
    "\n",
    "print(train_data.shape, (train_data[\"Label\"]==1).mean())\n",
    "print(valid_data.shape, (valid_data[\"Label\"]==1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Can_data</th>\n",
       "      <th>Section</th>\n",
       "      <th>Id</th>\n",
       "      <th>sen_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>504601</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The second period mainly includes modern CTD d...</td>\n",
       "      <td>Nordic Seas and</td>\n",
       "      <td>Temporal development</td>\n",
       "      <td>e26d2c87-b41e-44ad-8af8-079648a9bab5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26517</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The approximate location of the center of the ...</td>\n",
       "      <td>Ajith H. Perera,</td>\n",
       "      <td>Spatial variation: habitat disturbances</td>\n",
       "      <td>22be8f26-7e69-4559-bf48-2bd13a5ada35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273254</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Section Three discusses the background of the ...</td>\n",
       "      <td>Section Three discusses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4f642b3e-da80-4e53-8a86-fde746bb21af</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773738</th>\n",
       "      <td>0.0</td>\n",
       "      <td>PISA is one of the world's largest educational...</td>\n",
       "      <td>PISA</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>95d21d82-310e-4ce7-9e5a-30bbdec1eb8d</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943276</th>\n",
       "      <td>0.0</td>\n",
       "      <td>http://dx.doi.org/10.1002/grl.50880  ical Rese...</td>\n",
       "      <td>Research Atmospheres,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5de064a4-925f-4076-8f70-4691c0bbf6bf</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517943</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Since these outcomes are counts truncated at z...</td>\n",
       "      <td>X. I include</td>\n",
       "      <td>III. Estimation Methods</td>\n",
       "      <td>28a1e9e6-9ea5-4a5e-8a33-c62873b71823</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Although U.S. data on high-skill migration con...</td>\n",
       "      <td>Although U.S. data</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>d58436ef-1479-4a86-aa71-6dd45e5f24d0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121213</th>\n",
       "      <td>0.0</td>\n",
       "      <td>A simulation model was developed to predict in...</td>\n",
       "      <td>Whitehall II participants</td>\n",
       "      <td>Simulation study</td>\n",
       "      <td>a0014e7b-99a6-4149-984e-79bbe72caafa</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439027</th>\n",
       "      <td>0.0</td>\n",
       "      <td>• International Health Terminology Standards D...</td>\n",
       "      <td>International Health Terminology Standards Dev...</td>\n",
       "      <td>C. HEALTHCARE STANDARDS FOR DATA INTEGRATION</td>\n",
       "      <td>3e79dc82-cabb-4b1c-8f08-3e54e6597754</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This document is covered by a signed \"Reproduc...</td>\n",
       "      <td>ERIC</td>\n",
       "      <td>Conclusion</td>\n",
       "      <td>3d97a7d1-2287-49ee-957b-9995df99921d</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1392462 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label                                           Sentence  \\\n",
       "504601     0.0  The second period mainly includes modern CTD d...   \n",
       "26517      0.0  The approximate location of the center of the ...   \n",
       "1273254    0.0  Section Three discusses the background of the ...   \n",
       "773738     0.0  PISA is one of the world's largest educational...   \n",
       "943276     0.0  http://dx.doi.org/10.1002/grl.50880  ical Rese...   \n",
       "...        ...                                                ...   \n",
       "1517943    0.0  Since these outcomes are counts truncated at z...   \n",
       "767236     0.0  Although U.S. data on high-skill migration con...   \n",
       "1121213    0.0  A simulation model was developed to predict in...   \n",
       "1439027    0.0  • International Health Terminology Standards D...   \n",
       "1927498    0.0  This document is covered by a signed \"Reproduc...   \n",
       "\n",
       "                                                  Can_data  \\\n",
       "504601                                     Nordic Seas and   \n",
       "26517                                     Ajith H. Perera,   \n",
       "1273254                            Section Three discusses   \n",
       "773738                                                PISA   \n",
       "943276                               Research Atmospheres,   \n",
       "...                                                    ...   \n",
       "1517943                                       X. I include   \n",
       "767236                                  Although U.S. data   \n",
       "1121213                          Whitehall II participants   \n",
       "1439027  International Health Terminology Standards Dev...   \n",
       "1927498                                               ERIC   \n",
       "\n",
       "                                              Section  \\\n",
       "504601                           Temporal development   \n",
       "26517         Spatial variation: habitat disturbances   \n",
       "1273254                                           NaN   \n",
       "773738                                   Introduction   \n",
       "943276                                            NaN   \n",
       "...                                               ...   \n",
       "1517943                       III. Estimation Methods   \n",
       "767236                                   Introduction   \n",
       "1121213                              Simulation study   \n",
       "1439027  C. HEALTHCARE STANDARDS FOR DATA INTEGRATION   \n",
       "1927498                                    Conclusion   \n",
       "\n",
       "                                           Id  sen_length  \n",
       "504601   e26d2c87-b41e-44ad-8af8-079648a9bab5          26  \n",
       "26517    22be8f26-7e69-4559-bf48-2bd13a5ada35          35  \n",
       "1273254  4f642b3e-da80-4e53-8a86-fde746bb21af          15  \n",
       "773738   95d21d82-310e-4ce7-9e5a-30bbdec1eb8d          47  \n",
       "943276   5de064a4-925f-4076-8f70-4691c0bbf6bf           7  \n",
       "...                                       ...         ...  \n",
       "1517943  28a1e9e6-9ea5-4a5e-8a33-c62873b71823          58  \n",
       "767236   d58436ef-1479-4a86-aa71-6dd45e5f24d0          44  \n",
       "1121213  a0014e7b-99a6-4149-984e-79bbe72caafa          27  \n",
       "1439027  3e79dc82-cabb-4b1c-8f08-3e54e6597754          23  \n",
       "1927498  3d97a7d1-2287-49ee-957b-9995df99921d          37  \n",
       "\n",
       "[1392462 rows x 6 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \n",
    "# print(can_dataset_extract[\"Label\"].mean())\n",
    "# can_dataset_extract[can_dataset_extract[\"Label\"]==1]\n",
    "\n",
    "# list(set(train_data[\"dataset_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if omit some data label\n",
    "\n",
    "# non_found_dataset = []\n",
    "# for dataset in list(set(train_data[\"dataset_label\"])):\n",
    "# #     print(\"======\")\n",
    "# #     print(dataset)\n",
    "#     flag = 0\n",
    "#     for val in can_dataset_extract[\"Can_data\"].values:\n",
    "#         if val.find(dataset) > -1 or dataset.find(val) > -1:\n",
    "# #             print(val)\n",
    "#             flag = 1\n",
    "#             break\n",
    "#     if flag == 0:\n",
    "#         non_found_dataset.append(dataset)\n",
    "\n",
    "# len(non_found_dataset), non_found_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counted = 0\n",
    "# for idex, tid in enumerate(title_ids):\n",
    "#     for val in id_title_dict[tid][\"section_title\"].values:\n",
    "#         if str(val).find(\"Data\") > -1 or str(val).find(\"data\") > -1 :\n",
    "#             counted += 1\n",
    "#             print(idex)\n",
    "#             break\n",
    "# counted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert vectorlize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "can_dataset_extract = pd.read_csv(\"data1.csv\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\",output_hidden_states = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n",
      "135000\n",
      "140000\n",
      "145000\n",
      "150000\n",
      "155000\n",
      "160000\n",
      "165000\n",
      "170000\n",
      "175000\n",
      "180000\n",
      "185000\n",
      "190000\n",
      "195000\n",
      "200000\n",
      "205000\n",
      "210000\n",
      "215000\n",
      "220000\n",
      "225000\n",
      "230000\n",
      "235000\n",
      "240000\n",
      "245000\n",
      "250000\n",
      "255000\n",
      "260000\n",
      "265000\n",
      "270000\n",
      "275000\n",
      "280000\n",
      "285000\n",
      "290000\n",
      "295000\n",
      "300000\n",
      "305000\n",
      "310000\n",
      "315000\n",
      "320000\n",
      "325000\n",
      "330000\n",
      "335000\n",
      "340000\n",
      "345000\n",
      "350000\n",
      "355000\n",
      "360000\n",
      "365000\n",
      "370000\n",
      "375000\n",
      "380000\n",
      "385000\n",
      "390000\n",
      "395000\n",
      "400000\n",
      "405000\n",
      "410000\n",
      "415000\n",
      "420000\n",
      "425000\n",
      "430000\n",
      "435000\n",
      "440000\n",
      "445000\n",
      "450000\n",
      "455000\n",
      "460000\n",
      "465000\n",
      "470000\n",
      "475000\n",
      "480000\n",
      "485000\n",
      "490000\n",
      "495000\n",
      "500000\n",
      "505000\n",
      "510000\n",
      "515000\n",
      "520000\n",
      "525000\n",
      "530000\n",
      "535000\n",
      "540000\n",
      "545000\n",
      "550000\n",
      "555000\n",
      "560000\n",
      "565000\n",
      "570000\n",
      "575000\n",
      "580000\n",
      "585000\n",
      "590000\n",
      "595000\n",
      "600000\n",
      "605000\n",
      "610000\n",
      "615000\n",
      "620000\n",
      "625000\n",
      "630000\n",
      "635000\n",
      "640000\n",
      "645000\n",
      "650000\n",
      "655000\n",
      "660000\n",
      "665000\n",
      "670000\n",
      "675000\n",
      "680000\n",
      "685000\n",
      "690000\n",
      "695000\n",
      "700000\n",
      "705000\n",
      "710000\n",
      "715000\n",
      "720000\n",
      "725000\n",
      "730000\n",
      "735000\n",
      "740000\n",
      "745000\n",
      "750000\n",
      "755000\n",
      "760000\n",
      "765000\n",
      "770000\n",
      "775000\n",
      "780000\n",
      "785000\n",
      "790000\n",
      "795000\n",
      "800000\n",
      "805000\n",
      "810000\n",
      "815000\n",
      "820000\n",
      "825000\n",
      "830000\n",
      "835000\n",
      "840000\n",
      "845000\n",
      "850000\n",
      "855000\n",
      "860000\n",
      "865000\n",
      "870000\n",
      "875000\n",
      "880000\n",
      "885000\n",
      "890000\n",
      "895000\n",
      "900000\n",
      "905000\n",
      "910000\n",
      "915000\n",
      "920000\n",
      "925000\n",
      "930000\n",
      "935000\n",
      "940000\n",
      "945000\n",
      "950000\n",
      "955000\n",
      "960000\n",
      "965000\n",
      "970000\n",
      "975000\n",
      "980000\n",
      "985000\n",
      "990000\n",
      "995000\n",
      "1000000\n",
      "1005000\n",
      "1010000\n",
      "1015000\n",
      "1020000\n",
      "1025000\n",
      "1030000\n",
      "1035000\n",
      "1040000\n",
      "1045000\n",
      "1050000\n",
      "1055000\n",
      "1060000\n",
      "1065000\n",
      "1070000\n",
      "1075000\n",
      "1080000\n",
      "1085000\n",
      "1090000\n",
      "1095000\n",
      "1100000\n",
      "1105000\n",
      "1110000\n",
      "1115000\n",
      "1120000\n",
      "1125000\n",
      "1130000\n",
      "1135000\n",
      "1140000\n",
      "1145000\n",
      "1150000\n",
      "1155000\n",
      "1160000\n",
      "1165000\n",
      "1170000\n",
      "1175000\n",
      "1180000\n",
      "1185000\n",
      "1190000\n",
      "1195000\n",
      "1200000\n",
      "1205000\n",
      "1210000\n",
      "1215000\n",
      "1220000\n",
      "1225000\n",
      "1230000\n",
      "1235000\n",
      "1240000\n",
      "1245000\n",
      "1250000\n",
      "1255000\n",
      "1260000\n",
      "1265000\n",
      "1270000\n",
      "1275000\n",
      "1280000\n",
      "1285000\n",
      "1290000\n",
      "1295000\n",
      "1300000\n",
      "1305000\n",
      "1310000\n",
      "1315000\n",
      "1320000\n",
      "1325000\n",
      "1330000\n",
      "1335000\n",
      "1340000\n",
      "1345000\n",
      "1350000\n",
      "1355000\n",
      "1360000\n",
      "1365000\n",
      "1370000\n",
      "1375000\n",
      "1380000\n",
      "1385000\n",
      "1390000\n",
      "1395000\n",
      "1400000\n",
      "1405000\n",
      "1410000\n",
      "1415000\n",
      "1420000\n",
      "1425000\n",
      "1430000\n",
      "1435000\n",
      "1440000\n",
      "1445000\n",
      "1450000\n",
      "1455000\n",
      "1460000\n",
      "1465000\n",
      "1470000\n",
      "1475000\n",
      "1480000\n",
      "1485000\n",
      "1490000\n",
      "1495000\n",
      "1500000\n",
      "1505000\n",
      "1510000\n",
      "1515000\n",
      "1520000\n",
      "1525000\n",
      "1530000\n",
      "1535000\n",
      "1540000\n",
      "1545000\n",
      "1550000\n",
      "1555000\n",
      "1560000\n",
      "1565000\n",
      "1570000\n",
      "1575000\n",
      "1580000\n",
      "1585000\n",
      "1590000\n",
      "1595000\n",
      "1600000\n",
      "1605000\n",
      "1610000\n",
      "1615000\n",
      "1620000\n",
      "1625000\n",
      "1630000\n",
      "1635000\n",
      "1640000\n",
      "1645000\n",
      "1650000\n",
      "1655000\n",
      "1660000\n",
      "1665000\n",
      "1670000\n",
      "1675000\n",
      "1680000\n",
      "1685000\n",
      "1690000\n",
      "1695000\n",
      "1700000\n",
      "1705000\n",
      "1710000\n",
      "1715000\n",
      "1720000\n",
      "1725000\n",
      "1730000\n",
      "1735000\n",
      "1740000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Got 202 and 262 in dimension 1 (The offending index is 1359)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-24cad08f2275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Convert the lists into tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mattention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcan_dataset_extract\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 202 and 262 in dimension 1 (The offending index is 1359)"
     ]
    }
   ],
   "source": [
    "def find_pos(a, b):\n",
    "    v = np.ones_like(b)\n",
    "    conv = np.convolve(a, v, \"valid\")\n",
    "    start = np.argmax(conv==b.sum())\n",
    "#     if (conv==b.sum()).sum() != 1:\n",
    "#         print(a, b)\n",
    "    return (start, start + b.shape[0])\n",
    "\n",
    "# data paperid candidate label\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids = []\n",
    "can_dataset_pos = []\n",
    "attention_masks = []\n",
    "allow_max_length = 202\n",
    "# For every sentence...\n",
    "for idx in range(can_dataset_extract.shape[0]):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    \n",
    "    can_dataset = can_dataset_extract[\"Can_data\"].values[idx].split(\" \")\n",
    "    sent = can_dataset_extract['Sentence'].values[idx].split(\" \")\n",
    "    encoded_dict1 = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        is_split_into_words=True,\n",
    "                        max_length = allow_max_length,           # Pad & truncate all sentences.\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    encoded_dict2 = tokenizer.encode_plus(\n",
    "                        can_dataset,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        is_split_into_words=True,\n",
    "                        max_length = 202,           # Pad & truncate all sentences.\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )   \n",
    "    \n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_id = encoded_dict1['input_ids']\n",
    "    input_ids.append(input_id)\n",
    "    \n",
    "    pattern = encoded_dict2[\"input_ids\"][0]\n",
    "    pattern = pattern[pattern>0][1:-1]\n",
    "\n",
    "    pos = find_pos(input_id[0][input_id[0]>0].numpy(), pattern.numpy())\n",
    "    can_dataset_pos.append(pos)\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    if idx % 5000 == 0:\n",
    "        print(idx)\n",
    "\n",
    "# # Convert the lists into tensors.\n",
    "# input_ids = torch.cat(input_ids, dim=0)\n",
    "# attention_masks = torch.cat(attention_masks, dim=0)\n",
    "# labels = torch.tensor(can_dataset_extract['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_ids = []\n",
    "new_can_dataset_pos = []\n",
    "new_attention_masks = []\n",
    "new_labels = []\n",
    "allow_max_length = 202\n",
    "\n",
    "for idx in range(len(input_ids)):\n",
    "    if input_ids[idx].shape[-1] == allow_max_length:\n",
    "        new_input_ids.append(input_ids[idx])\n",
    "        new_can_dataset_pos.append(can_dataset_pos[idx])\n",
    "        new_attention_masks.append(attention_masks[idx])\n",
    "        new_labels.append(can_dataset_extract[\"Label\"].values[idx])\n",
    "        \n",
    "input_ids = new_input_ids\n",
    "can_dataset_pos = new_can_dataset_pos\n",
    "attention_masks = new_attention_masks\n",
    "labels = new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 2019/1729723 [01:07<11:32:50, 41.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "term_dict = {\"id\": input_ids,\n",
    "             \"can_dataset_pos\": can_dataset_pos, \n",
    "             \"attention_masks\": attention_masks,\n",
    "             \"label\": labels}\n",
    "\n",
    "import pickle\n",
    "pickle.dump(term_dict, open(\"term_input_ids.pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in bash shell\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda:0\")\n",
    "candicate_embeddings = []\n",
    "n = len(input_ids)\n",
    "with torch.no_grad():  # 将输入传入模型，得到每一层的输出信息，这里的encoded_layers为12层，可以打印验证\n",
    "    for i in range(n):\n",
    "        outputs = model(input_ids[i].to(\"cuda:0\"))\n",
    "        hidden_states = outputs[2]\n",
    "        \n",
    "        token_embeddings = torch.cat(hidden_states, dim=0) # 13 * sen_len * 768\n",
    "        # Swap dimensions 0 and 1.\n",
    "        token_embeddings = token_embeddings.permute(1, 0, 2)# sen_len * 13 * 768\n",
    "        # For each token in the sentence...\n",
    "        token_vecs_sum = torch.sum(token_embeddings[:,-4:,:], dim=1)\n",
    "        start, end = can_dataset_pos[i]\n",
    "        candicate_embeddings.append(token_vecs_sum[start:end].cpu().numpy())\n",
    "            \n",
    "        if i % 5000 == 0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "term2_dict = {\"can_embeddings\": candicate_embeddings,\n",
    "              \"labels\": labels}\n",
    "\n",
    "pickle.dump(term2_dict, open(\"can_embeddings.pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
